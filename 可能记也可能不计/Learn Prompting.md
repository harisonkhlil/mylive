# Learn Prompting

#ai #prompt

## 基础

### Prompt 组成部分

* 角色
* 指令/prompt
* question
* 上下文
* 范例提示

下面两个问题，其中第二个提示更好原因是：**指令是提示的最后一部分，语言模型更倾向于按照指令输入输出上下文例如，如果给定第一个提示，语言模型可能会回复：`2022年3月15日:与神经科医生预约随访，以评估脑震荡恢复进展`，而不是直接预测患者的风险。**

```prompt
Q:你是一名医生。请阅读这份病史并预测患者的风险：

2000年1月1日：打篮球时右臂骨折。戴上石膏进行治疗。
2010年2月15日：被诊断为高血压。开了利辛普利的处方。
2015年9月10日：患上肺炎。用抗生素治疗并完全康复。
2022年3月1日：在一次车祸中患上脑震荡。被送进医院接受24小时的监护。
```

```prompt
2000年1月1日：打篮球时右臂骨折。戴上石膏进行治疗。
2010年2月15日：被诊断为高血压。开了利辛普利的处方。
2015年9月10日：患上肺炎。用抗生素治疗并完全康复。
2022年3月1日：在一次车祸中患上脑震荡。被送进医院接受24小时的监护。

你是一名医生。请阅读这份病史并预测患者的风险：
```

==多范例提示能生成准确且格式正确的回答==

## LLMs（Large Language Modules）中的隐患

* 引用来源

* 偏见（一般没有偏见，但是应该会有一定的刻板印象，即使有安全防护措施，有时候也会生成性别歧视 / 种族歧视的相关内容

  